---
title: Integrieren von Apache Kafka Connect in Azure Event Hubs mit Debezium für Change Data Capture
description: Dieser Artikel enthält Informationen zur Verwendung von Debezium mit Azure Event Hubs für Kafka.
ms.topic: how-to
author: abhirockzz
ms.author: abhishgu
ms.date: 10/18/2021
ms.openlocfilehash: b2c39e3b8b408ddb6a3efe6641e8a537a180eb2d
ms.sourcegitcommit: 677e8acc9a2e8b842e4aef4472599f9264e989e7
ms.translationtype: HT
ms.contentlocale: de-DE
ms.lasthandoff: 11/11/2021
ms.locfileid: "132278225"
---
# <a name="integrate-apache-kafka-connect-support-on-azure-event-hubs-with-debezium-for-change-data-capture"></a>Integrieren der Unterstützung für Apache Kafka Connect in Azure Event Hubs mit Debezium für Change Data Capture

Bei **Change Data Capture (CDC)** handelt es sich um eine Methode, die verwendet wird, um Änderungen auf Zeilenebene in Datenbanktabellen als Reaktion auf Erstellungs-, Aktualisierungs- und Löschvorgänge nachzuverfolgen. [Debezium](https://debezium.io/) ist eine verteilte Plattform, die auf den in verschiedenen Datenbanken verfügbaren Change Data Capture-Funktionen aufbaut (z. B. [logische Decodierung in PostgreSQL](https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html)). Es bietet eine Reihe von [Kafka Connect-Connectors](https://debezium.io/documentation/reference/1.2/connectors/index.html), die auf Änderungen auf Zeilenebene in Datenbanktabellen zugreifen und diese in Ereignisstreams konvertieren, die dann an [Apache Kafka](https://kafka.apache.org/) gesendet werden.

> [!WARNING]
> Die Verwendung des Apache Kafka Connect-Frameworks sowie der Debezium-Plattform und der zugehörigen Connectors ist **nicht für den Produktsupport durch Microsoft Azure qualifiziert**.
>
> Apache Kafka Connect setzt für die dynamische Konfiguration voraus, dass sie in komprimierten Themen mit ansonsten unbegrenzter Aufbewahrungsdauer gespeichert wird. Event Hubs [implementiert keine Komprimierung als Brokerfeature](event-hubs-federation-overview.md#log-projections) und erzwingt stets einen zeitbasierten Aufbewahrungsgrenzwert für beibehaltene Ereignisse. Dies beruht auf dem Prinzip, dass es sich bei Event Hubs um eine Echtzeit-Engine zum Streamen von Ereignissen handelt und nicht um einen langfristigen Daten- oder Konfigurationsspeicher.
>
> Während das Apache Kafka-Projekt mit der Vermischung dieser Rollen durchaus zurechtkommt, vertritt Azure die Überzeugung, dass solche Informationen am besten in einer eigenen Datenbank oder einem Konfigurationsspeicher verwaltet werden.
>
> Viele Apache Kafka Connect-Szenarien sind funktionsfähig, aber diese konzeptionellen Unterschiede zwischen den Aufbewahrungsmodellen von Apache Kafka und Event Hubs können dazu führen, dass bestimmte Konfigurationen nicht erwartungsgemäß funktionieren. 

In diesem Tutorial werden Sie schrittweise durch den Vorgang geführt, mit dem Sie ein Change Data Capture-basiertes System in Azure mithilfe von [Event Hubs](./event-hubs-about.md?WT.mc_id=devto-blog-abhishgu) (für Kafka), [Azure DB for PostgreSQL](../postgresql/overview.md) und Debezium einrichten. Es verwendet den [Debezium PostgreSQL-Connector](https://debezium.io/documentation/reference/1.2/connectors/postgresql.html), um Datenbankänderungen aus PostgreSQL zu Kafka-Themen in Event Hubs zu streamen.

> [!NOTE]
> Dieser Artikel enthält Verweise auf den Begriff *Whitelist*, den Microsoft nicht länger verwendet. Sobald der Begriff aus der Software entfernt wurde, wird er auch aus diesem Artikel entfernt.

In diesem Tutorial führen Sie die folgenden Schritte aus:

> [!div class="checklist"]
> * Erstellen eines Event Hubs-Namespace
> * Einrichten und Konfigurieren von Azure Database for PostgreSQL
> * Konfigurieren und Ausführen von Kafka Connect mit dem PostgreSQL-Connector von Debezium
> * Testen von Change Data Capture
> * (Optional) Nutzen von Datenänderungsereignissen mit einem `FileStreamSink`-Connector

## <a name="pre-requisites"></a>Voraussetzungen
Im Rahmen dieser exemplarischen Vorgehensweise ist Folgendes erforderlich:

- Azure-Abonnement. Falls Sie nicht über ein Abonnement verfügen, können Sie ein [kostenloses Konto erstellen](https://azure.microsoft.com/free/).
- Linux/macOS
- Kafka-Release (Version 1.1.1, Scala-Version 2.11), verfügbar unter [kafka.apache.org](https://kafka.apache.org/downloads#1.1.1)
- Lesen Sie den Einführungsartikel [Event Hubs für Apache Kafka](./event-hubs-for-kafka-ecosystem-overview.md).

## <a name="create-an-event-hubs-namespace"></a>Erstellen eines Event Hubs-Namespace
Ein Event Hubs-Namespace ist erforderlich, um Nachrichten an einen Event Hubs-Dienst zu senden und von diesem zu empfangen. Anweisungen zum Erstellen eines Namespace und eines Event Hub finden Sie unter [Erstellen eines Event Hubs](event-hubs-create.md). Rufen Sie die Event Hubs-Verbindungszeichenfolge und den vollqualifizierten Domänennamen (Fully Qualified Domain Name, FQDN) zur späteren Verwendung ab. Anweisungen hierzu finden Sie unter [Get an Event Hubs connection string](event-hubs-get-connection-string.md) (Abrufen einer Event Hubs-Verbindungszeichenfolge). 

## <a name="set-up-and-configure-azure-database-for-postgresql"></a>Einrichten und Konfigurieren von Azure Database for PostgreSQL
[Azure Database for PostgreSQL](../postgresql/overview.md) ist ein relationaler Datenbankdienst, der auf der Communityversion des Open-Source-Datenbankmoduls PostgreSQL basiert und in zwei Bereitstellungsoptionen verfügbar ist: „Einzelserver“ und „Hyperscale (Citus)“ [Befolgen Sie diese Anweisungen](../postgresql/quickstart-create-server-database-portal.md), um einen Azure Database for PostgreSQL-Server über das Azure-Portal zu erstellen. 

## <a name="setup-and-run-kafka-connect"></a>Einrichten und Ausführen von Kafka Connect
In diesem Abschnitt werden die folgenden Themen behandelt:

- Installation des Debezium-Connectors
- Konfigurieren von Kafka Connect für Event Hubs
- Starten des Kafka Connect-Clusters mit dem Debezium-Connector

### <a name="download-and-setup-debezium-connector"></a>Herunterladen und Einrichten des Debezium-Connectors
Befolgen Sie die neuesten Anweisungen in der [Debezium-Dokumentation](https://debezium.io/documentation/reference/1.2/connectors/postgresql.html#postgresql-deploying-a-connector), um den Connector herunterzuladen und einzurichten.

- Laden Sie das Plug-In-Archiv des Connectors herunter. Um beispielsweise die Version `1.2.0` des Connectors herunterzuladen, verwenden Sie diesen Link: https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.2.0.Final/debezium-connector-postgres-1.2.0.Final-plugin.tar.gz.
- Extrahieren Sie die JAR-Dateien, und kopieren Sie sie in den [Kafka Connect-Plug-In-Pfad](https://kafka.apache.org/documentation/#connectconfigs).


### <a name="configure-kafka-connect-for-event-hubs"></a>Konfigurieren von Kafka Connect für Event Hubs
Es ist nur eine minimale Neukonfiguration erforderlich, wenn Sie den Kafka Connect-Durchsatz von Kafka an Event Hubs umleiten.  Im folgenden Beispiel `connect-distributed.properties` ist dargestellt, wie Sie Connect konfigurieren, um die Authentifizierung und Kommunikation mit dem Kafka-Endpunkt unter Event Hubs einzurichten:

> [!IMPORTANT]
> - Debezium erstellt automatisch ein Thema pro Tabelle und eine Reihe von Metadatenthemen. Das Kafka-**Thema** entspricht einer Event Hubs-Instanz (Event Hub). Weitere Zuordnungen zwischen Apache Kafka und Azure Event Hubs finden Sie unter [Konzeptionelle Zuordnung zwischen Kafka und Event Hubs](event-hubs-for-kafka-ecosystem-overview.md#kafka-and-event-hub-conceptual-mapping). 
> - Je nach Tarif (Basic, Standard, Premium oder Dedicated) gelten unterschiedliche **Grenzwerte** für die Anzahl von Event Hubs in einem Event Hubs-Namespace. Informationen zu diesen Grenzwerten finden Sie unter [Kontingente](compare-tiers.md#quotas).

```properties
bootstrap.servers={YOUR.EVENTHUBS.FQDN}:9093 # e.g. namespace.servicebus.windows.net:9093
group.id=connect-cluster-group

# connect internal topic names, auto-created if not exists
config.storage.topic=connect-cluster-configs
offset.storage.topic=connect-cluster-offsets
status.storage.topic=connect-cluster-status

# internal topic replication factors - auto 3x replication in Azure Storage
config.storage.replication.factor=1
offset.storage.replication.factor=1
status.storage.replication.factor=1

rest.advertised.host.name=connect
offset.flush.interval.ms=10000

key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
internal.key.converter=org.apache.kafka.connect.json.JsonConverter
internal.value.converter=org.apache.kafka.connect.json.JsonConverter

internal.key.converter.schemas.enable=false
internal.value.converter.schemas.enable=false

# required EH Kafka security settings
security.protocol=SASL_SSL
sasl.mechanism=PLAIN
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="$ConnectionString" password="{YOUR.EVENTHUBS.CONNECTION.STRING}";

producer.security.protocol=SASL_SSL
producer.sasl.mechanism=PLAIN
producer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="$ConnectionString" password="{YOUR.EVENTHUBS.CONNECTION.STRING}";

consumer.security.protocol=SASL_SSL
consumer.sasl.mechanism=PLAIN
consumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="$ConnectionString" password="{YOUR.EVENTHUBS.CONNECTION.STRING}";

plugin.path={KAFKA.DIRECTORY}/libs # path to the libs directory within the Kafka release
```

> [!IMPORTANT]
> Ersetzen Sie `{YOUR.EVENTHUBS.CONNECTION.STRING}` durch die Verbindungszeichenfolge für Ihren Event Hubs-Namespace. Anweisungen zum Abrufen der Verbindungszeichenfolge finden Sie unter [Abrufen einer Event Hubs-Verbindungszeichenfolge](event-hubs-get-connection-string.md). Hier sehen Sie eine Beispielkonfiguration: `sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="$ConnectionString" password="Endpoint=sb://mynamespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=XXXXXXXXXXXXXXXX";`


### <a name="run-kafka-connect"></a>Ausführen von Kafka Connect
In diesem Schritt wird ein Kafka Connect-Worker lokal im verteilten Modus gestartet, indem Event Hubs zum Warten des Clusterzustands verwendet wird.

1. Speichern Sie die obige Datei `connect-distributed.properties` lokal.  Achten Sie darauf, alle Werte in geschweiften Klammern zu ersetzen.
2. Navigieren Sie zum Speicherort des Kafka-Release auf Ihrem Computer.
3. Führen Sie `./bin/connect-distributed.sh /PATH/TO/connect-distributed.properties` aus, und warten Sie, bis der Cluster startet.

> [!NOTE]
> Kafka Connect verwendet die Kafka-AdminClient-API, um automatisch Themen mit empfohlenen Konfigurationen, einschließlich Komprimierung, zu erstellen. Eine schnelle Überprüfung des Namespace im Azure-Portal ergibt, dass die internen Themen des Connect-Workers automatisch erstellt wurden.
>
> Interne Kafka Connect-Themen **müssen Komprimierung verwenden**.  Das Event Hubs-Team ist nicht für die Korrektur falscher Konfigurationen zuständig, sollten interne Connect-Themen nicht ordnungsgemäß konfiguriert sein.

### <a name="configure-and-start-the-debezium-postgresql-source-connector"></a>Konfigurieren und Starten des Debezium PostgreSQL-Quellconnectors

Erstellen Sie eine Konfigurationsdatei (`pg-source-connector.json`) für den PostgreSQL-Quellconnector, und ersetzen Sie dabei die Werte gemäß Ihrer Azure PostgreSQL-Instanz.

```json
{
    "name": "todo-connector",
    "config": {
        "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
        "database.hostname": "<replace with Azure PostgreSQL instance name>.postgres.database.azure.com",
        "database.port": "5432",
        "database.user": "<replace with database user name>",
        "database.password": "<replace with database password>",
        "database.dbname": "postgres",
        "database.server.name": "my-server",
        "plugin.name": "wal2json",
        "table.whitelist": "public.todos"
    }
}
```

> [!TIP]
> Das `database.server.name`-Attribut ist ein logischer Name, der einen Namespace für den spezifischen überwachten PostgreSQL-Datenbankserver/-Cluster identifiziert und bereitstellt. Ausführliche Informationen finden Sie in der [Debezium-Dokumentation](https://debezium.io/documentation/reference/1.2/connectors/postgresql.html#postgresql-property-database-server-name).

Verwenden Sie zum Erstellen einer Instanz des Connectors den Kafka Connect-REST-API-Endpunkt:

```bash
curl -X POST -H "Content-Type: application/json" --data @pg-source-connector.json http://localhost:8083/connectors
```

So überprüfen Sie den Status des Connectors

```bash
curl -s http://localhost:8083/connectors/todo-connector/status
```

## <a name="test-change-data-capture"></a>Testen von Change Data Capture
Um Change Data Capture in Aktion zu sehen, müssen Sie Datensätze in der Azure PostgreSQL-Datenbank erstellen/aktualisieren/löschen.

Beginnen Sie damit, dass Sie eine Verbindung mit Ihrer Azure PostgreSQL-Datenbank herstellen (im folgenden Beispiel wird [psql](https://www.postgresql.org/docs/12/app-psql.html) verwendet).

```bash
psql -h <POSTGRES_INSTANCE_NAME>.postgres.database.azure.com -p 5432 -U <POSTGRES_USER_NAME> -W -d <POSTGRES_DB_NAME> --set=sslmode=require

e.g. 

psql -h my-postgres.postgres.database.azure.com -p 5432 -U testuser@my-postgres -W -d postgres --set=sslmode=require
```

**Erstellen einer Tabelle und Einfügen von Datensätzen**

```sql
CREATE TABLE todos (id SERIAL, description VARCHAR(50), todo_status VARCHAR(12), PRIMARY KEY(id));

INSERT INTO todos (description, todo_status) VALUES ('setup postgresql on azure', 'complete');
INSERT INTO todos (description, todo_status) VALUES ('setup kafka connect', 'complete');
INSERT INTO todos (description, todo_status) VALUES ('configure and install connector', 'in-progress');
INSERT INTO todos (description, todo_status) VALUES ('start connector', 'pending');
```

Der Connector sollte jetzt aktiv werden und Datenänderungsereignisse an ein Event Hubs-Thema mit dem Namen `my-server.public.todos` senden, vorausgesetzt, Sie haben `my-server` als Wert für `database.server.name` festgelegt, und `public.todos` ist die Tabelle, deren Änderungen Sie nachverfolgen (gemäß Konfiguration `table.whitelist`).

**Überprüfen des Event Hubs-Themas**

Betrachten wir nun den Inhalt des Themas, um sicherzustellen, dass alles wie erwartet funktioniert. Im folgenden Beispiel wird [`kafkacat`](https://github.com/Azure/azure-event-hubs-for-kafka/tree/master/quickstart/kafkacat) verwendet, aber Sie können auch [einen Consumer mithilfe einer der hier aufgeführten Optionen erstellen](apache-kafka-developer-guide.md).

Erstellen Sie eine Datei namens `kafkacat.conf` mit folgendem Inhalt:

```
metadata.broker.list=<enter event hubs namespace>.servicebus.windows.net:9093
security.protocol=SASL_SSL
sasl.mechanisms=PLAIN
sasl.username=$ConnectionString
sasl.password=<enter event hubs connection string>
```

> [!NOTE]
> Aktualisieren Sie die Attribute `metadata.broker.list` und `sasl.password` in `kafkacat.conf` gemäß den Event Hubs-Informationen. 

Starten Sie in einem anderen Terminal einen Consumer:

```bash
export KAFKACAT_CONFIG=kafkacat.conf
export BROKER=<enter event hubs namespace>.servicebus.windows.net:9093
export TOPIC=my-server.public.todos

kafkacat -b $BROKER -t $TOPIC -o beginning
```

Die JSON-Nutzdaten, die die Datenänderungsereignisse darstellen, die als Reaktion auf die Zeilen, die Sie in der Tabelle `todos` hinzugefügt haben, in PostgreSQL generiert wurden, sollten angezeigt werden. Hier ist ein Codeausschnitt der Nutzdaten:


```json
{
    "schema": {...},
    "payload": {
        "before": null,
        "after": {
            "id": 1,
            "description": "setup postgresql on azure",
            "todo_status": "complete"
        },
        "source": {
            "version": "1.2.0.Final",
            "connector": "postgresql",
            "name": "fullfillment",
            "ts_ms": 1593018069944,
            "snapshot": "last",
            "db": "postgres",
            "schema": "public",
            "table": "todos",
            "txId": 602,
            "lsn": 184579736,
            "xmin": null
        },
        "op": "c",
        "ts_ms": 1593018069947,
        "transaction": null
    }
```

Das Ereignis besteht aus der `payload` zusammen mit dem `schema` (aus Gründen der Übersichtlichkeit ausgelassen). Beachten Sie im Abschnitt `payload`, wie der Erstellungsvorgang (`"op": "c"`) dargestellt wird: `"before": null` bedeutet, dass es sich um eine neu eingefügte (`INSERT`) Zeile handelte; `after` stellt Werte für die Spalten in der Zeile bereit; `source` stellt die PostgreSQL-Instanzmetadaten von dem Ort bereit, an dem dieses Ereignis abgerufen wurde, usw.

Sie können dasselbe auch mit Aktualisierungs- oder Löschvorgängen ausprobieren und die Datenänderungsereignisse untersuchen. Um z. B. den Aufgabenstatus für `configure and install connector` zu aktualisieren (vorausgesetzt, seine `id` ist `3`):

```sql
UPDATE todos SET todo_status = 'complete' WHERE id = 3;
```

## <a name="optional-install-filestreamsink-connector"></a>(Optional) Installieren eines FileStreamSink-Connectors
Nachdem nun alle Änderungen an der Tabelle `todos` in dem Event Hubs-Thema aufgezeichnet werden, verwenden Sie den FileStreamSink-Connector (der standardmäßig in Kafka Connect verfügbar ist), um diese Ereignisse zu nutzen.

Erstellen Sie eine Konfigurationsdatei (`file-sink-connector.json`) für den Connector, und ersetzen Sie dabei das `file`-Attribut gemäß Ihrem Dateisystem.

```json
{
    "name": "cdc-file-sink",
    "config": {
        "connector.class": "org.apache.kafka.connect.file.FileStreamSinkConnector",
        "tasks.max": "1",
        "topics": "my-server.public.todos",
        "file": "<enter full path to file e.g. /Users/foo/todos-cdc.txt>"
    }
}
```

So erstellen Sie den Connector und überprüfen seinen Status

```bash
curl -X POST -H "Content-Type: application/json" --data @file-sink-connector.json http://localhost:8083/connectors

curl http://localhost:8083/connectors/cdc-file-sink/status
```

Datensätze einfügen/aktualisieren/löschen und die Datensätze in der konfigurierten Ausgabesenkendatei überwachen:

```bash
tail -f /Users/foo/todos-cdc.txt
```


## <a name="cleanup"></a>Cleanup
Mit Kafka Connect werden Event Hub-Themen zum Speichern von Konfigurationen, Offsets und des Status erstellt, die auch dann beibehalten werden, nachdem der Connect-Cluster heruntergefahren wurde. Wenn diese Beibehaltung nicht erwünscht ist, sollten diese Themen gelöscht werden. Sie können auch den Event Hub `my-server.public.todos` löschen, der in dieser exemplarischen Vorgehensweise erstellt wurde.

## <a name="next-steps"></a>Nächste Schritte

Weitere Informationen zu Event Hubs für Kafka finden Sie in folgenden Artikeln:  

- [Spiegeln eines Kafka-Brokers in einem Event Hub](event-hubs-kafka-mirror-maker-tutorial.md)
- [Verbinden von Apache Spark mit einem Event Hub](event-hubs-kafka-spark-tutorial.md)
- [Verbinden von Apache Flink mit einem Event Hub](event-hubs-kafka-flink-tutorial.md)
- [Erkunden von Beispielen auf GitHub](https://github.com/Azure/azure-event-hubs-for-kafka)
- [Verbinden von Akka Streams mit einem Event Hub](event-hubs-kafka-akka-streams-tutorial.md)
- [Apache Kafka-Entwicklerleitfaden für Azure Event Hubs](apache-kafka-developer-guide.md)
