### YamlMime:FAQ
metadata:
  title: FAQ über Azure Video Analyzer für Medien
  description: In diesem Artikel finden Sie häufig gestellte Fragen zu Azure Video Analyzer.
  ms.service: azure-video-analyzer
  ms.topic: conceptual
  ms.date: 11/04/2021
  ms.openlocfilehash: 4824e1dbf569ee21d897f7ce3ef7cd2649cf33e4
  ms.sourcegitcommit: 8946cfadd89ce8830ebfe358145fd37c0dc4d10e
  ms.translationtype: HT
  ms.contentlocale: de-DE
  ms.lasthandoff: 11/05/2021
  ms.locfileid: "131845432"
title: FAQ über den Azure Video Analyzer
summary: Dieser Artikel bietet Antworten auf häufig gestellte Fragen zu Azure Video Analyzer.
sections:
- name: Wird ignoriert.
  questions:
  - question: Allgemein
    answer: "**Welche Systemvariablen kann ich in der Definition der Pipeline-Topologie verwenden?**\n\n| Variable   |  BESCHREIBUNG  | \n| --- | --- | \n| System.Runtime.DateTime | Stellt einen Zeitpunkt (in UTC) dar, der üblicherweise als Datum und Uhrzeit im folgenden Format ausgedrückt wird:<br>*jjjjMMttTHHmmssZ* | \n| System.Runtime.PreciseDateTime | Stellt eine DateTime-Instanz in koordinierter Weltzeit (UTC) in einem ISO8601-konformen Format mit Millisekunden im folgenden Format dar:<br>*jjjjMMttTHHmmss.fffZ* | \n| System.TopologyName    | Stellt den Namen der Pipelinetopologie dar. | \n| System.PipelineName | Stellt den Namen der Livepipeline dar. | \n\n> [!Note] \n> System.Runtime.DateTime und System.Runtime.PreciseDateTime können nicht als Teil des Namens einer Azure Video Analyzer-Videoressource in einem Knoten der Videosenke verwendet werden. Diese Variablen können in einem Datei-Sink-Knoten für die Benennung der Datei verwendet werden.\n\n**Wie lautet die Datenschutzrichtlinie für Video Analyzer?**\n\nVideo Analyzer ist Teil der [Datenschutzbestimmungen von Microsoft](https://privacy.microsoft.com/privacystatement). Die Datenschutzerklärung erläutert die von Microsoft verarbeiteten personenbezogenen Daten sowie die Art und die Zwecke der Verarbeitung dieser Daten durch Microsoft. Weitere Informationen zum Datenschutz finden Sie im [Microsoft Trust Center](https://www.microsoft.com/trustcenter).\n"
  - question: Konfiguration und Bereitstellung
    answer: >
      **Kann ich das Edgemodul auf einem Windows 10-Gerät bereitstellen?**


      Ja. Weitere Informationen finden Sie unter [Linux-Container unter Windows 10](/virtualization/windowscontainers/deploy-containers/linux-containers).


      **Ich verwende einen virtuellen Computer als simuliertes Edgegerät. Kann ich die Größe der VM-Konfiguration nach der Bereitstellung ändern?**


      Ja, Sie können [eine VM auf eine andere Größe skalieren](../../../virtual-machines/resize-vm.md), nachdem Edge-Module wie Video Analyzer für sie bereitgestellt wurden. Möglicherweise müssen Sie die IoT Edge-Laufzeitumgebung auf Ihrer virtuellen Maschine neu starten. Verwenden Sie für einen virtuellen Linux-Computer den Befehl `sudo systemctl restart iotedge`. Wenn Livepipelines ausgeführt wurden, sollten sie nach Abschluss des Neustarts fortgesetzt werden. Während des Neustarts kommt es zu Unterbrechungen in der Ausgabe der Pipeline (Beispiel: Inferenzereignisse, Videoaufzeichnung).


      **Wie kann ich dem RTSP-Kamera-Simulator-Modul auf meinem IoT Edge-Gerät Beispiel-Videodateien hinzufügen?**


      Wenn Sie den Kamerasimulator mit Hilfe einer Schnellstartanleitung oder eines Tutorials installiert haben, können Sie eine neue Videodatei hinzufügen:


      - Prüfen Sie, ob die Datei in einem [unterstützten Format](http://www.live555.com/mediaServer/) vorliegt und den H.264-Videocodec verwendet

      - Laden Sie die Datei auf `/home/localedgeuser/samples/input/<videofile.mkv>` Ihr IoT Edge-Gerät herunter, wie [in diesem Abschnitt](analyze-ai-composition.md#review-the-video-sample) gezeigt


      Beispielmediendateien, die zum Testen verfügbar sind, finden Sie unter [Mediendatensets](https://github.com/Azure/video-analyzer/tree/main/media).
  - question: Aufzeichnen von IP-Kameras und RTSP-Einstellungen
    answer: "**Muss ich auf meinem Gerät ein spezielles SDK verwenden, um einen Videostream zu senden?**\n\nNein. Video Analyzer unterstützt das Aufzeichnen von Medien mithilfe des Videostreamingprotokolls RTSP (Real-Time Streaming Protocol) für Videostreams, das von den meisten IP-Kameras unterstützt wird.\n\n**Kann ich Medien mit anderen Protokollen als RTSP zum Video Analyzer pushen?**\n\nNein. Video Analyzer unterstützt für die Aufzeichnung von Videos von IP-Kameras nur RTSP. Alles Kameras, die das RTSP-Streaming über TCP/HTTP unterstützen, sollten funktionieren. \n\n**Kann ich die URL der RTSP-Quelle in einer Livepipeline zurücksetzen oder aktualisieren?**\n\nJa, sofern sich die Livepipeline in einem *inaktiven* Zustand befindet.  \n\n**Ist ein RTSP-Simulator verfügbar, der bei Tests und während der Entwicklung verwendet werden kann?**\n\nJa, es steht ein [RTSP-Simulator](https://github.com/Azure/video-analyzer/tree/main/edge-modules/sources/rtspsim-live555) als Edgemodul zur Verfügung, das Sie in den Schnellstarts und Tutorials zur Unterstützung des Lernprozesses verwenden können. Dieses Modul wird nach bestem Wissen bereitgestellt und ist möglicherweise nicht immer verfügbar. Es wird dringend empfohlen, den Simulator *nicht* länger als einige Stunden zu verwenden. Sie sollten Tests mit Ihrer eigentlichen RTSP-Quelle durchführen, bevor Sie eine Produktionsbereitstellung planen.\n"
  - question: Entwerfen Ihres KI-Modells
    answer: "**Ich habe mehrere KI-Modelle in einem Docker-Container verpackt. Wie sollte ich sie mit Video Analyzer verwenden?** \n\nDie Lösungen hängen von dem Kommunikationsprotokoll ab, das der Inferencing-Server für die Kommunikation mit Video Analyzer verwendet. In den folgenden Abschnitten wird die Funktionsweise der einzelnen Protokolle beschrieben.\n\n*Verwenden des HTTP-Protokolls:*\n\n* Einzelner Container (Modul mit dem Namen *avaextension):*  \n\n   In Ihrem Rückschlussserver können Sie einen einzelnen Port, aber unterschiedliche Endpunkte für verschiedene KI-Modelle verwenden. Beispielsweise können Sie für ein Python-Beispiel verschiedene `routes` pro Modell wie folgt verwenden: \n\n   ```\n   @app.route('/score/face_detection', methods=['POST']) \n   … \n   Your code specific to face detection model\n\n   @app.route('/score/vehicle_detection', methods=['POST']) \n   … \n   Your code specific to vehicle detection model \n   … \n   ```\n\n   Und dann legen Sie in Ihrer Video Analyzer-Bereitstellung bei der Aktivierung von Live-Pipelines die Inferenzserver-URL für jede einzelne wie hier gezeigt fest: \n\n   Erste Livepipeline: Rückschlussserver-URL=`http://avaextension:44000/score/face_detection`<br/>\n   Zweite Livepipeline: Rückschlussserver-URL=`http://avaextension:44000/score/vehicle_detection`  \n   \n    > [!NOTE]\n    > Alternativ können Sie Ihre KI-Modelle auf verschiedenen Ports exponieren und sie aufrufen, wenn Sie Live-Pipelines aktivieren.  \n\n* Mehrere Container: \n\n   Jeder Container wird mit einem anderen Namen bereitgestellt. In den Schnellstarts und Tutorials haben wir Ihnen gezeigt, wie Sie eine Erweiterung namens *avaextension* bereitstellen. Nun können Sie zwei unterschiedliche Container entwickeln, die jeweils dieselbe HTTP-Schnittstelle und somit auch denselben `/score`-Endpunkt aufweisen. Stellen Sie diese beiden Container mit unterschiedlichen Namen bereit, und stellen Sie sicher, dass beide an *anderen Ports* lauschen. \n\n   Beispielsweise lauscht ein Container mit dem Namen `avaextension1` an Port `44000`, der andere Container mit dem Namen `avaextension2` an Port `44001`. \n\n   In Ihrer Video Analyzer-Topologie instanziieren Sie zwei Live-Pipelines mit unterschiedlichen Inferenz-URLs, wie hier gezeigt: \n\n   Erste Livepipeline: Rückschlussserver-URL = `http://avaextension1:44000/score`    \n   Zweite Livepipeline: Rückschlussserver-URL=`http://avaextension2:44001/score`\n   \n*Verwenden des gRPC-Protokolls:* \n\n* Der gRPC-Erweiterungsknoten hat eine Eigenschaft `extensionConfiguration`, eine optionale Zeichenfolge, die als Teil des gRPC-Vertrags verwendet werden kann. Wenn Sie mehrere KI-Modelle in einem einzelnen Rückschlussserver gepackt haben, müssen Sie nicht für jedes KI-Modell einen Knoten verfügbar machen. Stattdessen können Sie als Erweiterungsanbieter für eine Livepipeline mithilfe der `extensionConfiguration`-Eigenschaft festlegen, wie die verschiedenen KI-Modelle ausgewählt werden. Während der Ausführung übergibt Azure Video Analyzer diese Zeichenfolge an Ihren Inferenzserver, der sie zum Aufrufen des gewünschten KI-Modells verwenden kann. \n\n**Ich erstelle einen gRPC-Server um ein KI-Modell herum und möchte die Verwendung durch mehrere Kameras/Livepipelines unterstützen. Wie sollte ich meinen Server erstellen?** \n\n Stellen Sie zunächst sicher, dass der Server mehrere Anforderungen gleichzeitig verarbeiten oder in parallelen Threads arbeiten kann. \n\nBeispielsweise wurde in einem der [Azure Video Analyzer gRPC-Beispiele](analyze-live-video-use-your-model-grpc.md) eine Standardanzahl paralleler Kanäle festgelegt: \n\n```\nserver = grpc.server(futures.ThreadPoolExecutor(max_workers=3)) \n```\n\nIn der obigen gRPC-Serverinstanziierung kann der Server nur drei Kanäle pro Kamera oder pro Livepipeline gleichzeitig öffnen. Sie sollten nicht versuchen, mehr als drei Instanzen mit dem Server zu verbinden. Wenn Sie versuchen, mehr als drei Kanäle zu öffnen, bleiben Anforderungen ausstehend, bis ein vorhandener Kanal getrennt wird.  \n\nDie obige gRPC-Serverimplementierung wird in den Python-Beispielen verwendet. Als Entwickler können Sie einen eigenen Server implementieren oder die vorherige Standardimplementierung verwenden, um die Workeranzahl zu erhöhen. Diese legen Sie auf die für Videofeeds verwendete Anzahl von Kameras fest. \n\nWenn Sie mehrere Kameras einrichten und verwenden möchten, können Sie mehrere Livepipelines instanziieren, wobei jede auf denselben oder einen anderen Rückschlussserver zeigt (z. B. auf den im obigen Absatz genannten Server). \n\n**Ich möchte mehrere Frames empfangen können, bevor ich eine Rückschlussentscheidung treffe. Wie kann ich dies ermöglichen?** \n\nDie aktuellen [Standardbeispiele](https://github.com/Azure/video-analyzer/tree/main/edge-modules) funktionieren im *zustandslosen* Modus. Sie behalten nicht den Status früherer Aufrufe oder die ID des Aufrufers bei. Dies bedeutet, dass mehrere Livepipelines denselben Rückschlussserver aufrufen können, der Server jedoch nicht unterscheiden kann, wer ihn aufruft oder welcher Zustand je Aufrufer gilt. \n\n*Verwenden des HTTP-Protokolls:*\n\nUm den Zustand beizubehalten, ruft jeder Aufrufer oder jede Livepipeline den Rückschlussserver mit einem für den Aufrufer eindeutigen HTTP-Abfrageparameter auf. Beispielsweise werden die Rückschlussserver-URLs für jede Livepipeline hier angezeigt:  \n\nErste Livepipeline: `http://avaextension:44000/score?id=1`<br/>\nZweite Livepipeline: `http://avaextension:44000/score?id=2`\n\n… \n\nServerseitig hilft `id`, den Aufrufer zu identifizieren. Bei `id` =1 kann der Server den Zustand für diese Livepipeline separat beibehalten. Er kann dann die empfangenen Videoframes in einem Puffer aufbewahren. Verwenden Sie z. B. ein Array oder ein Wörterbuch mit einem DateTime-Schlüssel, dessen Wert der Frame ist. Anschließend können Sie den Server für die Verarbeitung (den Rückschluss) definieren, nachdem *x* Frames empfangen wurden. \n\n*Verwenden des gRPC-Protokolls:* \n\nBei einer gRPC-Erweiterung ist jede Sitzung für einen einzelnen Kamerafeed vorgesehen, sodass kei Bezeichner bereitgestellt werden muss. Mit der extensionConfiguration-Eigenschaft können Sie die Videoframes in einem Puffer speichern und den Verarbeitungsserver (Rückschlussserver) definieren, nachdem *x* Frames empfangen wurden. \n\n**Führen alle ProcessMediaStreams in einem bestimmten Container dasselbe KI-Modell aus?** \n\nNein. Das Starten/Beenden von Aufrufen durch den Endbenutzer bei einer Livepipeline stellt eine Sitzung dar, oder möglicherweise wird eine Kameraverbindung getrennt bzw. erneut hergestellt. Das Ziel besteht darin, während des Videostreamings durch die Kamera eine Sitzung aufrechtzuerhalten. \n\n* Zwei Kameras, die Video zur Verarbeitung (an zwei separate Livepipelines) senden, erstellen zwei Sitzungen. \n* Wenn eine Kamera mit einer Livepipeline verbunden ist, die über zwei gRPC-Erweiterungsknoten verfügt, werden zwei Sitzungen erstellt. \n\nJede Sitzung ist eine Vollduplexverbindung zwischen Video Analyzer und dem gRPC-Server, und jede Sitzung kann ein anderes Modell aufweisen. \n\n> [!NOTE]\n> Wenn eine Kamera die Verbindung trennt oder wiederherstellt und dabei für einen außerhalb der Toleranzgrenzen liegenden Zeitraum offline geschaltet wird, öffnet Video Analyzer eine neue Sitzung mit dem gRPC-Server. Der Server muss den Zustand nicht über diese Sitzungen hinweg nachverfolgen. \n\nVideo Analyzer wurde außerdem die Unterstützung mehrerer gRPC-Erweiterungen für eine einzelne Kamera in einer Livepipeline hinzugefügt. Sie können diese gRPC-Erweiterungen verwenden, um die KI-Verarbeitung sequenziell oder parallel bzw. sogar in einer Kombination aus beidem auszuführen. \n\n> [!NOTE]\n> Das parallele Ausführen mehrerer Erweiterungen wirkt sich auf Ihre Hardwareressourcen aus. Behalten Sie dies im Hinterkopf, wenn Sie die Hardware für Ihre Rechenanforderungen auswählen. \n\n**Was ist die maximale Anzahl gleichzeitiger ProcessMediaStreams?** \n\nVideo Analyzer wendet auf diese Anzahl keine Einschränkungen an.  \n\n**Wie kann ich entscheiden, ob der Rückschlussserver CPU oder GPU bzw. einen anderen Hardwarebeschleuniger verwenden soll?** \n\nIhre Entscheidung hängt von der Komplexität des entwickelten KI-Modells und der gewünschten Verwendung der CPU- und Hardwarebeschleunigung ab. Bei der Entwicklung des KI-Modells können Sie angeben, welche Ressourcen vom Modell verwendet werden sollen und welche Aktionen es durchführen soll. \n\n**Wie zeiche ich die von meinem Rückschlussserver generierten Begrenzungsfelder an?** \n\nSie können die Rückschlussergebnisse zusammen mit den Medien in Ihrer Videoressource aufzeichnen. Sie können ein [Widget](../player-widget.md) verwenden, um das Video mit einer Überlagerung der Rückschlussdaten abzuspielen.\n"
  - question: GRPC-Kompatibilität
    answer: "**Woher weiß ich, welche Pflichtfelder der Mediendatenstrom-Deskriptor benötigt?** \n\nJedes Feld, für das Sie keinen Wert angeben, erhält einen [von gRPC vorgegebenen Standardwert](https://developers.google.com/protocol-buffers/docs/proto3#default).  \n\nVideo Analyzer verwendet die Version *proto3* der Protokollpuffersprache. Alle Protokollpufferdaten, die von Video Analyzer genutzt werden, sind in den [Protokollpufferdateien]() verfügbar<!--add-valid-link.md)--><!--https://github.com/Azure/azree-video-analyzer/tree/master/contracts/grpc-->. \n\n**Wie kann ich sicherstellen, dass ich die aktuellen Protokollpufferdateien verwende?** \n\nSie können die aktuellen Protokollpufferdateien auf der [Website mit den Vertragsdateien](https://github.com/Azure/video-analyzer/tree/main/contracts/grpc) abrufen. Wenn wir die Vertragsdateien aktualisieren, werden sie an diesem Ort angezeigt. Obwohl es keinen unmittelbaren Plan zum Aktualisieren der Protokolldateien gibt, suchen Sie nach dem Paketnamen am Anfang der Dateien, um die Version zu ermitteln. sollte aber wie folgt lauten: \n\n```\nmicrosoft.azure.media.live_video_analytics.extensibility.grpc.v1\n```\n\nBei allen Updates dieser Dateien wird der „v-Wert“ am Ende des Namens heraufgesetzt. \n\n> [!NOTE]\n> Da in Video Analyzer die Version „proto3“ der Sprache verwendet wird, sind die Felder optional. Dies sorgt für Ab- und Aufwärtskompatibilität. \n\n**Welche gRPC-Features sind für die Verwendung mit Video Analyzer verfügbar? Welche Features sind obligatorisch und welche optional?** \n\nSie können alle serverseitigen gRPC-Funktionen verwenden, wenn der Protokollpuffervertrag (Protobuf) erfüllt ist.\n"
  - question: Überwachung und Metriken
    answer: "**Kann ich die Pipeline am Edge mithilfe von Azure Event Grid überwachen?**\n\nJa. Sie können die [Prometheus-Metriken](monitor-log-edge.md#azure-monitor-collection-via-telegraf) nutzen und in Event Grid veröffentlichen. \n\n**Kann ich Azure Monitor verwenden, um Integrität, Metriken und Leistung meiner Pipelines in der Cloud oder am Edge anzuzeigen?**\n\nJa, diese Vorgehensweise wird unterstützt. Weitere Informationen finden Sie in der [Übersicht über Azure Monitor-Metriken](../../../azure-monitor/essentials/data-platform-metrics.md).\n\n**Gibt es Tools, die die Überwachung des Video Analyzer Edge-Moduls erleichtern?**\n\nVisual Studio Code unterstützt die Erweiterung „Azure IoT Tools“, mit der Sie die Endpunkte des Video-Analyzer-Edgemoduls problemlos überwachen können. Sie können dieses Tool verwenden, um schnell mit der Überwachung des integrierten IoT Hub-Endpunkts auf „Ereignisse“ zu beginnen und die Rückschlussmeldungen anzuzeigen, die vom Edgegerät an die Cloud weitergeleitet werden. \n\nDarüber hinaus können Sie mit dieser Erweiterung den Modulzwilling für das Video Analyzer-Edgemoduls und damit die Einstellungen der Pipeline bearbeiten.\n\nWeitere Informationen finden Sie im Artikel [Überwachung und Protokollierung](monitor-log-edge.md).\n"
  - question: Abrechnung und Verfügbarkeit
    answer: >
      **Wie wird Video Analyzer abgerechnet?**


      Abrechnungsdetails finden Sie unter [Azure Video Analyzer – Preise](https://azure.microsoft.com/pricing/details/video-analyzer/).
additionalContent: "\n## <a name=\"next-steps\"></a>Nächste Schritte\n\n[Schnellstart: Erste Schritte mit Azure Video Analyzer](get-started-detect-motion-emit-events.md)"
